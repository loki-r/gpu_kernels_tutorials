# AI Kernel Tutorials

This repository hosts a collection of AI kernel tutorials tailored for various hardware platforms. The tutorials are organized by hardware target and then by lesson complexity, starting from basic operators and progressing to more complex models like transformers. Our goal is to provide a structured learning path for developers looking to optimize AI workloads on specific hardware.

## Repository Structure

The repository is organized as follows:

### `nvidia/`
- **`01_basic_operators/`**
  - `softmax.ipynb`
  - `sigmoid.ipynb`
- **`02_simple_layers/`**
  - `mlp.ipynb`
  - `glu.ipynb`
- **`03_attention_layers/`**
  - `attention.ipynb`
- **`04_transformer_model/`**
  - `transformer.ipynb`

### `qualcomm_npu/`
- **`01_basic_operators/`**
  - `softmax.ipynb`
  - `sigmoid.ipynb`
- **`02_simple_layers/`**
  - `mlp.ipynb`
  - `glu.ipynb`
- **`03_attention_layers/`**
  - `attention.ipynb`
- **`04_transformer_model/`**
  - `transformer.ipynb`

### `apple/`
- **`01_basic_operators/`**
  - `softmax.ipynb`
  - `sigmoid.ipynb`
- **`02_simple_layers/`**
  - `mlp.ipynb`
  - `glu.ipynb`
- **`03_attention_layers/`**
  - `attention.ipynb`
- **`04_transformer_model/`**
  - `transformer.ipynb`

### `mediatek/`
- **`01_basic_operators/`**
  - `softmax.ipynb`
  - `sigmoid.ipynb`
- **`02_simple_layers/`**
  - `mlp.ipynb`
  - `glu.ipynb`
- **`03_attention_layers/`**
  - `attention.ipynb`
- **`04_transformer_model/`**
  - `transformer.ipynb`

Each `.ipynb` file contains a tutorial focused on a specific operator or model component for the respective hardware.
